~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
QHN  2组实验

QHN_MLP

       模型：三层的MLP  （64，128）（128，128）（128，1）
       数据集：自定义的DXS1  一共25600个样本，8:2划分为训练集和验证集
       优化算法：SGD、Momentum、NAG、RNAG、QHM、QHN

             1 对SGD进行步长搜索。固定预热阶段为10个epoch，步长从0.1步长线性增长到1步长。每30epoch执行一次0.2的学习率退火。
                步长搜索范围为：0.5，0.7，1，1.2，1.5，最佳步长为1
             2 其余算法的参数搜索。在SGD表现最好的步长上对Momentum、NAG、RNAG、QHM、QHN进行参数搜索，其中
                Momentum、NAG、RNAG搜索范围为β=[0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 0.995, 0.999, 0.9995]
                QHM、QHN的搜索范围为β=[0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 0.995, 0.999, 0.9995]，v=[0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99, 0.995, 0.999]
             3 在Momentum、NAG、RNAG、QHM、QHN最好的参数下进行步长搜索，与1类似。
                最佳参数：Momentum[0.95]、NAG[0.95]、RNAG[0.95]、QHM[0.99, 0.95]、QHN[0.99, 0.95]

       结果：
             1 绘制在SGD最佳步长下SGD、Momentum、NAG、RNAG、QHM、QHN的最佳参数的对比（训练损失和验证精度  2张）
             2 绘制QHM和QHN在β=0.999下不同v=[0.5, 0.7, 0.9, 0.99]的对比（训练损失  1张）
             3 绘制QHM(β=0.999, v=0.7)和QHN(β=0.999, v=0.8)在不同lr=[0.1, 0.2, 0.3, 0.5, 0.7, 1]的对比（训练损失  1张）

QHN_CNN

       模型：GLeNet 两层卷积三层全连接
       数据集：FMNIST
       优化算法：SGD、Momentum、NAG、RNAG、QHM、QHN

             1 对SGD进行步长搜索。固定预热阶段为10个epoch，步长从0.1步长线性增长到1步长。每30epoch执行一次0.2的学习率退火。
                步长搜索范围为：0.1，0.15，0.2，0.3，0.4，0.5，0.6，0.7，0.8，0.9，1， 1.1， 1.2， 1.3， 1.4， 1.5
             2 其余算法的参数搜索。在SGD表现最好的步长上对Momentum、NAG、RNAG、QHM、QHN进行参数搜索，其中
                Momentum、NAG、RNAG搜索范围为β=[0.8, 0.9, 0.95, 0.99, 0.995]
                QHM、QHN的搜索范围为β=[0.9, 0.95, 0.99, 0.995, 0.999, 0.9995]，v=[0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.99]
                搜索结果：Momentum[0.9]、NAG[0.9]、RNAG[0.9]、QHM[0.999, 0.7]、QHN[0.999, 0.8]
             3 在Momentum、NAG、RNAG、QHM、QHN最好的参数下进行步长搜索，与1类似。

       结果：
             1 绘制在SGD最佳步长下SGD、Momentum、NAG、RNAG、QHM、QHN的最佳参数的对比（训练损失和验证精度  2张）
             2 绘制QHM和QHN在β=0.999下不同v=[0.5, 0.7, 0.9, 0.99]的对比（训练损失  1张）
             3 绘制QHM(β=0.999, v=0.7)和QHN(β=0.999, v=0.8)在不同lr=[0.1, 0.2, 0.3, 0.5, 0.7, 1]的对比（训练损失  1张）


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
AdaQHN  2组

AdaQHN_CNN
       
       模型：GLeNet 两层卷积三层全连接
       数据集：FMNIST
       优化算法：Adam、Adam_win、Adan、QHAdam、AdaQHN

                固定预热阶段为10个epoch，步长从0.1步长线性增长到1步长。每30epoch执行一次0.2的学习率退火。

                固定各算法参数分别为 
                Adam[β1=0.9, β2=0.999]    Adam_win[β1=0.9, β2=0.999, wd=1e-4]    Adan[β1=0.98, β2=0.92, β3=0.99, wd=1e-4]
                QHAdam[β1=0.999, β2=0.999, v=0.7]    AdaQHN[β1=0.999, β2=0.999, v=0.8]

                首先对lr=[0.001, 0.005, 0.01, 0.05, 0.1]进行搜索，确定更细的lr搜索区间，假如Adam在0.005最好，那么搜索区间为[0.001, 0.01]
                第一次搜索结果，所有算法均为均为0.001或0.005，区间选择[0.001, 0.01]

                再次对  lr=[0.001, 0.002, 0.003, 0.005, 0.007, 0.01]  进行搜索，确定最优lr
                第二次搜索结果  Adam、Adam_win、Adan、QHAdam、AdaQHN分别为[0.002, 0.002, 0.005, 0.003, 0.003]

       结果：
             1 绘制Adam、Adam_win、Adan、QHAdam、AdaQHN在其最佳lr下的对比图（训练损失和验证精度2张）


AdaQHN_RNN
       
       模型：GRU 单个隐藏层
       数据集：PTB
       优化算法：Adam、Adam_win、Adan、QHAdam、AdaQHN

                固定预热阶段为10个epoch，步长从0.1步长线性增长到1步长。每30epoch执行一次0.2的学习率退火。

                固定各算法参数分别为 
                Adam[β1=0.9, β2=0.999]    Adam_win[β1=0.9, β2=0.999, wd=1e-4]    Adan[β1=0.98, β2=0.92, β3=0.99, wd=1e-4]
                QHAdam[β1=0.999, β2=0.999, v=0.7]    AdaQHN[β1=0.999, β2=0.999, v=0.8]

                首先对lr=[0.001, 0.005, 0.01, 0.05, 0.1]进行搜索，确定更细的lr搜索区间，假如Adam在0.005最好，那么搜索区间为[0.001, 0.01]
                第一次搜索结果，0.001，0.005和0.01均有，区间选择[0.001, 0.02]

                再次对  lr=[0.001, 0.002, 0.003, 0.005, 0.007, 0.01, 0.015, 0.02]  进行搜索，确定最优lr
                第二次搜索结果  Adam、Adam_win、Adan、QHAdam、AdaQHN分别为[0.003, 0.003, 0.01, 0.007, 0.007]

       结果：
             1 绘制Adam、Adam_win、Adan、QHAdam、AdaQHN在其最佳lr下的对比图（训练损失和验证精度2张）


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Ada_algorithm_QHN  2组

Ada_algorithm_QHN  CNN
       
       模型：GLeNet 两层卷积三层全连接
       数据集：FMNIST
       优化算法：Adabelief、Adabelief_QHN、YOGI、YOGI_QHN

                固定预热阶段为10个epoch，步长从0.1步长线性增长到1步长。每30epoch执行一次0.2的学习率退火。

                固定各算法参数分别为 
                Adabelief[β1=0.9, β2=0.999]    Adabelief_QHN[β1=0.999, β2=0.999,v=0.8]
                YOGI[β1=0.9, β2=0.999]    YOGI_QHN[β1=0.999, β2=0.999,v=0.8]

                首先对lr=[0.001, 0.005, 0.01, 0.05, 0.1]进行搜索，确定更细的lr搜索区间
                第二次搜索区间均为 lr=[0.001, 0.0015, 0.002, 0.003, 0.005, 0.007, 0.01]
                第二次搜索结果  Adabelief、Adabelief_QHN、YOGI、YOGI_QHN分别为[0.002, 0.007, 0.007, 0.007]

       结果：
             1 绘制Adabelief、Adabelief_QHN、YOGI、YOGI_QHN在其最佳lr下的对比图（训练损失和验证精度2张）


Ada_algorithm_QHN  RNN
       
       模型：GRU 单个隐藏层
       数据集：PTB
       优化算法：Adam、Adam_win、Adan、QHAdam、AdaQHN

                固定预热阶段为10个epoch，步长从0.1步长线性增长到1步长。每30epoch执行一次0.2的学习率退火。

                固定各算法参数分别为 
                Adabelief[β1=0.9, β2=0.999]    Adabelief_QHN[β1=0.999, β2=0.999,v=0.8]
                YOGI[β1=0.9, β2=0.999]    YOGI_QHN[β1=0.999, β2=0.999,v=0.8]

                首先对lr=[0.001, 0.005, 0.01, 0.05, 0.1]进行搜索，确定更细的lr搜索区间
                第二次搜索区间分别为
                Adabelief  lr=[0.001, 0.002, 0.003, 0.005, 0.007, 0.01]
                Adabelief_QHN  lr=[0.01, 0.02, 0.03, 0.05, 0.07, 0.1]
                YOGI  lr=[0.005, 0.007, 0.01, 0.02, 0.03, 0.05]
                YOGI_QHN  lr=[0.005, 0.007, 0.01, 0.02, 0.03, 0.05]
                第二次搜索结果  Adabelief、Adabelief_QHN、YOGI、YOGI_QHN分别为[0.003, 0.05, 0.01, 0.01]

       结果：
             1 绘制Adabelief、Adabelief_QHN、YOGI、YOGI_QHN在其最佳lr下的对比图（训练损失和验证精度2张）

